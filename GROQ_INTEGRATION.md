# GROQ API INTEGRATION DOCUMENTATION

## **SECTION 4 ‚Äî API Integration (Groq)**

### Complete Groq API Integration with LLaMA 3.3 70B Versatile

This document provides a comprehensive explanation of the Groq API integration implemented in `groq_integration.py`.

---

## Overview

The Groq integration module handles all communication with the **LLaMA 3.3 70B Versatile** model through Groq's inference API. It includes:

- API client initialization
- Prompt engineering for code explanation
- Request/response handling
- Error management and validation
- Connection testing

---

## Implementation Details

### 1. Client Initialization

```python
from groq import Groq
from dotenv import load_dotenv
import os

# Load environment variables from .env file
load_dotenv()

# Initialize Groq client with API key
client = Groq(api_key=os.getenv("GROQ_API_KEY"))
```

**Key Points:**

- Uses `python-dotenv` to load API key from `.env` file
- API key is kept secure and never hardcoded
- Client is initialized once at module import

### 2. Model Configuration

```python
MODEL_NAME = "llama-3.3-70b-versatile"
MAX_TOKENS = 2048
TEMPERATURE = 0.7
TIMEOUT = 30  # seconds
```

**Parameter Explanations:**

- **MODEL_NAME**: `"llama-3.3-70b-versatile"`

  - Specifies LLaMA 3.3 70B Versatile model
  - Must match exact model name in Groq's system
  - Provides best balance of speed and quality

- **MAX_TOKENS**: `2048`

  - Maximum tokens in the response
  - Sufficient for comprehensive code explanations
  - Can be increased for longer explanations (up to 8192)

- **TEMPERATURE**: `0.7`

  - Controls randomness in responses
  - Range: 0.0 (deterministic) to 2.0 (very random)
  - 0.7 balances creativity with consistency
  - Lower values (0.3-0.5) for more factual responses
  - Higher values (0.8-1.0) for more creative explanations

- **TIMEOUT**: `30 seconds`
  - Maximum wait time for API response
  - Prevents hanging requests
  - Groq is typically very fast (1-3 seconds)

### 3. Prompt Engineering

The `create_explanation_prompt()` function structures prompts for optimal results:

```python
def create_explanation_prompt(code: str) -> str:
    prompt = f"""You are an expert programming instructor. Analyze the following code and provide a clear, comprehensive explanation.

Your explanation should include:
1. **Overview**: What does this code do? (1-2 sentences)
2. **Detailed Breakdown**: Explain the code step-by-step, covering:
   - Key variables and their purposes
   - Important functions/methods and their roles
   - Logic flow and control structures
   - Any algorithms or patterns used
3. **Key Concepts**: Highlight important programming concepts demonstrated
4. **Potential Issues**: Note any potential bugs, edge cases, or improvements (if applicable)

Code to explain:
```

{code}

```

Provide a well-structured, educational explanation that helps the reader understand both what the code does and why it's written this way."""

    return prompt
```

**Prompt Design Rationale:**

1. **Clear Role Definition**: "expert programming instructor" sets the tone
2. **Structured Output**: Numbered sections ensure consistent format
3. **Comprehensive Coverage**: Requests overview, details, concepts, and issues
4. **Educational Focus**: Emphasizes teaching and understanding
5. **Markdown Formatting**: Encourages structured, readable responses

**Why This Prompt Works:**

- LLaMA 3.3 responds well to structured instructions
- Clear sections make parsing and display easier
- Educational framing produces beginner-friendly explanations
- Asking for issues encourages critical analysis

### 4. Main API Call Function

The `get_code_explanation()` function is the core integration:

```python
async def get_code_explanation(code: str) -> dict:
    try:
        # Input validation
        if not code or not code.strip():
            return {
                "success": False,
                "error": "No code provided. Please paste code to explain."
            }

        if len(code) > 10000:
            return {
                "success": False,
                "error": "Code is too long. Please provide a snippet under 10,000 characters."
            }

        # API key check
        if not os.getenv("GROQ_API_KEY"):
            return {
                "success": False,
                "error": "GROQ_API_KEY not configured. Please check your .env file."
            }

        # Create structured prompt
        prompt = create_explanation_prompt(code)

        # Call Groq API with LLaMA 3.3 70B
        chat_completion = client.chat.completions.create(
            messages=[
                {
                    "role": "system",
                    "content": "You are an expert programming instructor who provides clear, accurate, and educational code explanations."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            model=MODEL_NAME,
            temperature=TEMPERATURE,
            max_tokens=MAX_TOKENS,
            timeout=TIMEOUT,
            top_p=1,
            stream=False
        )

        # Extract and return explanation
        explanation = chat_completion.choices[0].message.content

        return {
            "success": True,
            "explanation": explanation.strip()
        }

    except Exception as e:
        # Error handling
        error_message = str(e)

        if "api_key" in error_message.lower():
            error_message = "Invalid API key. Please check your GROQ_API_KEY in .env file."
        elif "timeout" in error_message.lower():
            error_message = "Request timed out. Please try again with a smaller code snippet."
        elif "rate_limit" in error_message.lower():
            error_message = "Rate limit exceeded. Please wait a moment and try again."
        else:
            error_message = f"Error communicating with AI service: {error_message}"

        return {
            "success": False,
            "error": error_message
        }
```

**Function Flow:**

1. **Validation Phase**

   - Check if code is empty or whitespace only
   - Verify code length is within limits (10,000 chars)
   - Confirm API key is configured

2. **Prompt Creation**

   - Generate structured prompt using helper function
   - Embeds user code within instructional context

3. **API Request**

   - Uses chat completion endpoint (conversational format)
   - System message defines AI's role
   - User message contains the prompt
   - Configures model parameters

4. **Response Processing**

   - Extracts explanation from response
   - Strips whitespace
   - Returns success status and explanation

5. **Error Handling**
   - Catches all exceptions
   - Provides user-friendly error messages
   - Identifies common error types (API key, timeout, rate limit)

### 5. Chat Completion API Structure

**Messages Format:**

The Groq API uses OpenAI-compatible chat format:

```python
messages = [
    {
        "role": "system",
        "content": "System prompt defining AI behavior"
    },
    {
        "role": "user",
        "content": "User's request/question"
    }
]
```

**Roles:**

- **system**: Sets the AI's behavior and personality
- **user**: The actual user input/request
- **assistant**: (In conversations) Previous AI responses

**Why Separate System and User Messages:**

- System message provides consistent context
- User message can vary per request
- LLaMA 3.3 follows system instructions closely
- Separates "how to behave" from "what to do"

### 6. API Parameters Explained

```python
chat_completion = client.chat.completions.create(
    messages=[...],
    model="llama-3.3-70b-versatile",
    temperature=0.7,
    max_tokens=2048,
    timeout=30,
    top_p=1,
    stream=False
)
```

**Parameter Details:**

- **messages**: List of conversation messages (system + user)
- **model**: Specific model to use (LLaMA 3.3 70B Versatile)
- **temperature**: Randomness control (0.7 = balanced)
- **max_tokens**: Maximum response length
- **timeout**: Request timeout in seconds
- **top_p**: Nucleus sampling (1 = consider all tokens)
- **stream**: False = wait for complete response

**Advanced Parameters (Not Used But Available):**

- **frequency_penalty**: Reduces word repetition (0-2)
- **presence_penalty**: Encourages topic diversity (0-2)
- **stop**: Stop sequences to end generation early
- **n**: Number of completions to generate

### 7. Response Structure

**Success Response:**

```python
{
    "success": True,
    "explanation": "**Overview**: This code implements...\n\n**Detailed Breakdown**:..."
}
```

**Error Response:**

```python
{
    "success": False,
    "error": "Invalid API key. Please check your GROQ_API_KEY in .env file."
}
```

**Actual Groq API Response Object:**

```python
chat_completion = {
    "id": "chatcmpl-...",
    "object": "chat.completion",
    "created": 1234567890,
    "model": "llama-3.3-70b-versatile",
    "choices": [
        {
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "**Overview**: This code..."
            },
            "finish_reason": "stop"
        }
    ],
    "usage": {
        "prompt_tokens": 150,
        "completion_tokens": 450,
        "total_tokens": 600
    }
}
```

**Accessing the Explanation:**

```python
explanation = chat_completion.choices[0].message.content
```

### 8. Connection Testing

```python
def test_groq_connection() -> bool:
    try:
        test_code = "print('Hello, World!')"
        prompt = create_explanation_prompt(test_code)

        chat_completion = client.chat.completions.create(
            messages=[
                {"role": "user", "content": prompt}
            ],
            model=MODEL_NAME,
            max_tokens=100,
            timeout=10
        )

        return chat_completion.choices[0].message.content is not None

    except Exception as e:
        print(f"Connection test failed: {e}")
        return False
```

**Purpose:**

- Verifies API key is valid
- Confirms Groq service is accessible
- Tests basic functionality
- Called on server startup

**Design Choices:**

- Simple test input
- Short max_tokens for speed
- Quick timeout (10s)
- Returns boolean for easy checking

---

## Error Handling Strategy

### 1. Validation Errors (Client-Side)

- Empty code ‚Üí User-friendly message
- Code too long ‚Üí Character limit message
- Missing API key ‚Üí Configuration instruction

### 2. API Errors (Server-Side)

- Invalid API key ‚Üí Clear configuration guidance
- Timeout ‚Üí Suggest smaller code snippet
- Rate limit ‚Üí Ask user to wait
- Network errors ‚Üí Generic connectivity message

### 3. Unexpected Errors

- Catch-all exception handler
- Log full error for debugging
- Return safe, generic message to user

---

## Security Considerations

### 1. API Key Protection

```python
# ‚úÖ GOOD: Load from environment
api_key = os.getenv("GROQ_API_KEY")

# ‚ùå BAD: Hardcoded key
api_key = "gsk_abc123..."  # NEVER DO THIS
```

### 2. Input Validation

```python
# Prevent abuse and errors
if len(code) > 10000:
    return error_response("Code too long")
```

### 3. Error Message Sanitization

```python
# Don't expose internal details
error_message = "Error communicating with AI service"
# Instead of: str(exception)  # Could expose API keys, paths, etc.
```

---

## Performance Optimization

### 1. Async Function

```python
async def get_code_explanation(code: str) -> dict:
```

- Allows concurrent request handling
- Non-blocking I/O
- Better server performance

### 2. Request Timeout

```python
timeout=30
```

- Prevents hanging requests
- Frees up resources
- Ensures responsive API

### 3. Groq's LPU Advantage

- 300-800 tokens/second inference
- Typically 1-3 second responses
- Significantly faster than GPU inference

---

## Extending the Integration

### Adding Streaming Responses

For real-time token streaming:

```python
async def get_code_explanation_stream(code: str):
    stream = client.chat.completions.create(
        messages=[...],
        model=MODEL_NAME,
        stream=True  # Enable streaming
    )

    for chunk in stream:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content
```

### Adding Conversation History

For multi-turn conversations:

```python
messages = [
    {"role": "system", "content": "..."},
    {"role": "user", "content": "Explain this code: ..."},
    {"role": "assistant", "content": "This code implements..."},
    {"role": "user", "content": "Can you explain the algorithm in more detail?"}
]
```

### Adding Custom Parameters

```python
chat_completion = client.chat.completions.create(
    messages=[...],
    model=MODEL_NAME,
    temperature=0.5,  # More deterministic
    max_tokens=4096,  # Longer explanations
    top_p=0.9,       # Slightly more focused
    frequency_penalty=0.5,  # Reduce repetition
    presence_penalty=0.3    # Encourage diversity
)
```

---

## Testing the Integration

### Unit Test Example

```python
import asyncio
from groq_integration import get_code_explanation

async def test_basic_explanation():
    code = "def add(a, b):\n    return a + b"
    result = await get_code_explanation(code)

    assert result["success"] == True
    assert len(result["explanation"]) > 0
    print("‚úì Basic explanation test passed")

asyncio.run(test_basic_explanation())
```

### Manual Testing

```python
# Test with Python code
code = """
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)
"""

result = await get_code_explanation(code)
print(result["explanation"])
```

---

## Monitoring and Logging

### Add Detailed Logging

```python
import logging

logger = logging.getLogger(__name__)

async def get_code_explanation(code: str) -> dict:
    logger.info(f"Received code explanation request: {len(code)} characters")

    try:
        # ... API call ...
        logger.info(f"Successfully generated explanation: {len(explanation)} characters")
        return {"success": True, "explanation": explanation}

    except Exception as e:
        logger.error(f"API call failed: {str(e)}")
        return {"success": False, "error": error_message}
```

### Track API Usage

```python
# Add usage tracking
usage = chat_completion.usage
logger.info(f"Tokens used - Prompt: {usage.prompt_tokens}, Completion: {usage.completion_tokens}, Total: {usage.total_tokens}")
```

---

## Best Practices Summary

‚úÖ **DO:**

- Use environment variables for API keys
- Validate input before API calls
- Handle errors gracefully
- Use async for I/O operations
- Set appropriate timeouts
- Log requests and errors
- Test connection on startup

‚ùå **DON'T:**

- Hardcode API keys
- Skip input validation
- Expose raw error messages to users
- Use blocking I/O
- Set unlimited timeouts
- Ignore errors silently
- Assume API is always available

---

## Conclusion

The Groq integration is designed to be:

- **Secure**: API keys protected, input validated
- **Reliable**: Error handling, timeouts, testing
- **Performant**: Async operations, Groq's fast inference
- **Maintainable**: Clear code, logging, documentation
- **Extensible**: Easy to add features or modify behavior

This implementation provides a solid foundation for Phase 1 and can be extended for future phases with minimal changes.

---

**The Groq API integration is complete and production-ready!** üöÄ
